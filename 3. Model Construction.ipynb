{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Analyzing and Predicting Player Count Trends in Online Games\n",
    "\n",
    "## Part 3: Model Construction\n",
    "\n",
    "We will be utilizing a model known as SARIMAX, or Seasonal Auto-Regressive Integrated Moving Average with Exogenous Regressors. At first, we will not be using exogenous regressors, but we will be eventually incorporating our 'event' column to assess the impact of an ongoing event to our predictor of choice.  \n",
    "\n",
    "### Modelling Goals\n",
    "\n",
    "We wish to predict 3 separate things:\n",
    "\n",
    "1. Number of players at a given time in the future\n",
    "2. Player growth over a given period\n",
    "3. Player growth at a given time.\n",
    "\n",
    "### Modelling Process for SARIMA and SARIMAX\n",
    "\n",
    "1. Transform Data\n",
    "    - Outlier Removal\n",
    "    - Discontinuity\n",
    "2. Determine Seasonality\n",
    "    - Spectral Analysis\n",
    "3. Stationarity\n",
    "    - ACF & PACF Analysis\n",
    "4. Model Construction\n",
    "5. Model Comparison\n",
    "\n",
    "### Modelling Process for Our Data\n",
    "\n",
    "We have to ask what exact model we want to construct. Would we want a model that is built on a single game and can only preedict that one game? Or should we build a model that takes an amalgamization of all games, and build a model based upon that? Or perhaps making a model based upon our control dataframe, Team Fortress 2, and seeing how this model performs on other games. \n",
    "\n",
    "1. Basic Modelling\n",
    "    - CSGO\n",
    "    - DOTA 2\n",
    "    - Rocket League\n",
    "    - Team Fortress 2\n",
    "2. Amalgamized Modelling\n",
    "    - Amalgamization Technique\n",
    "    - Modelling\n",
    "    - Testing\n",
    "3. Control Model\n",
    "    - Team Fortress 2 modelling\n",
    "    - Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# we import the necessary libraries\n",
    "\n",
    "\n",
    "from scipy.stats import boxcox as bc\n",
    "import scipy.stats\n",
    "import itertools\n",
    "import statistics as stats\n",
    "from fsds.imports import * \n",
    "from datetime import datetime\n",
    "import statsmodels as sm\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Retreival"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we import our dataframes\n",
    "\n",
    "csgo = pd.read_csv('data/Clean/csgo.csv')\n",
    "dota = pd.read_csv('data/Clean/dota.csv')\n",
    "rl = pd.read_csv('data/Clean/rl.csv')\n",
    "tf = pd.read_csv('data/Clean/tf.csv')\n",
    "\n",
    "raw_ls = [csgo, dota, rl, tf]\n",
    "\n",
    "ls = raw_ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# we also need to drop unnecessary columns\n",
    "\n",
    "ls = list(map(lambda df: df.drop(columns = ['Unnamed: 0', 'index']), ls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and set our 'time' column to be the index\n",
    "\n",
    "for df in ls:\n",
    "    df.set_index(df['time'], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Basic Modelling\n",
    "\n",
    "### a. Preparation\n",
    "\n",
    "#### Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we make a placeholder list\n",
    "\n",
    "ls_one_a = ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need to transform data before we even decide on our predictor column. Let's inspect players over time first. \n",
    "\n",
    "\n",
    "for df in ls:\n",
    "    df_plot = df.copy()\n",
    "\n",
    "    df_plot = df_plot.drop(columns = ['viewers', 'event', '%chg_players', '%chg_viewers'])\n",
    "    df_plot = df_plot.drop(columns = ['time'])\n",
    "\n",
    "    df_plot.plot(figsize = (15,6))\n",
    "    plt.show()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we handle discontinuities\n",
    "\n",
    "def fill_zeroes(df, col):\n",
    "    for i in range(len(df)):\n",
    "        if df[col][i] == 0:\n",
    "            df[col][i] = df[col][i-1]\n",
    "    return df\n",
    "\n",
    "ls = list(map(lambda df: fill_zeroes(df, col = 'players'), ls))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and inspect to ensure this was successful\n",
    "\n",
    "for i, df in enumerate(ls):\n",
    "    df_plot = df.copy()\n",
    "    df_plot.set_index(df_plot['time'], inplace = True)\n",
    "    df_plot = df_plot.drop(columns = ['viewers', 'event', '%chg_players', '%chg_viewers'])\n",
    "    df_plot = df_plot.drop(columns = ['time'])\n",
    "\n",
    "    df_plot.plot(figsize = (15,6))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all of these games had major changes in number of players directly after their release, leading us to \n",
    "# model only player data a sufficient time after release. \n",
    "\n",
    "start_dates = [ \"2016-01-01\",\n",
    "              \"2016-01-01\",\n",
    "              \"2016-01-01\",\n",
    "              \"2013-01-01\"]\n",
    "\n",
    "trimmed_df_list = []\n",
    "\n",
    "for i in range(0, 4):\n",
    "    trimmed = ls[i].where(ls[i]['time'] >= start_dates[i]).dropna()\n",
    "    trimmed_df_list.append(trimmed)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we inspect our trimmed data\n",
    "\n",
    "for i, df in enumerate(trimmed_df_list):\n",
    "    df_plot = df.copy()\n",
    "    df_plot.set_index(df_plot['time'], inplace = True)\n",
    "    df_plot = df_plot.drop(columns = ['viewers', 'event', '%chg_players', '%chg_viewers'])\n",
    "    df_plot = df_plot.drop(columns = ['time'])\n",
    "\n",
    "    df_plot.plot(figsize = (15,6))\n",
    "    plt.show()\n",
    "\n",
    "ls_trimmed = trimmed_df_list\n",
    "\n",
    "ls = trimmed_df_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking much better, but we have more work to complete before this can be modelled. \n",
    "#### Stationarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# we visualize our mean player count over time. If this plot has a trend, our data is not stationary. \n",
    "\n",
    "for df in ls:\n",
    "    \n",
    "    rolling_mean = df['players'].rolling(window = 3).mean()\n",
    "    \n",
    "    fig = plt.figure(figsize = (15, 8))\n",
    "   # plt.plot(df['players'], color = 'blue', label = 'Players')\n",
    "    plt.plot(rolling_mean, color = 'orange', label = 'Rolling Average')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "None of our games have stationary data, and we can address this in multiple ways. We can complete data transformations such as rolling mean transformations or differencing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in ls:\n",
    "    df['players_logged'] = np.log(df['players'])\n",
    "    fig = plt.figure(figsize = (15, 8))\n",
    "    plt.plot(df['players_logged'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rolling Mean Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can stationize our data by subtracting the rolling mean from our 'players' column. This will force our data to be centered\n",
    "# on the mean, even if the mean changes over time. \n",
    "\n",
    "for df in ls:\n",
    "    rolling_mean = df['players'].rolling(window = 3).mean()\n",
    "    df_plot = df.copy()\n",
    "    df_plot['players_sub_mean'] = df['players'] - rolling_mean\n",
    "    df_plot.dropna(inplace = True)\n",
    "    fig = plt.figure(figsize = (15,8))\n",
    "    plt.plot(df_plot['players_sub_mean'], label = 'Players Centered on Rolling Average')\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data looks beautifully stationary now, though there are severe outliers in some cases. We'll address this after checking how differencing will make our data look.\n",
    "#### Differencing\n",
    "\n",
    "We have already constructed a column that represents a differencing technique. Our '%chg_players' represents this difference in the form of a percentage, but let's construct the raw difference for each games player counts. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using pandas diff function\n",
    "\n",
    "for df in ls:\n",
    "    df['difference'] = df['players'].diff(periods = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# basic differences\n",
    "\n",
    "for df in ls:\n",
    "    fig = plt.figure(figsize = (15, 8))\n",
    "    plt.plot(df['difference'], label = 'Differences in Players')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# differences of the rolling mean, though this will affect model interpretability\n",
    "\n",
    "for df in ls:\n",
    "    df['rolling_mean'] = df['players'].rolling(window = 3).mean()\n",
    "    df['rolling_mean_diff'] = df['rolling_mean'].diff(periods = 1)\n",
    "    fig = plt.figure(figsize = (15, 8))\n",
    "    plt.plot(df['rolling_mean_diff'], label = 'Differences in Rolling Mean')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# differences in logged data\n",
    "\n",
    "for df in ls:\n",
    "    \n",
    "    df['players_logged'] = np.log(df['players'])\n",
    "    df['players_logged_diff'] = df['players_logged'].diff(periods = 1)\n",
    "    \n",
    "    fig = plt.figure(figsize = (15, 8))\n",
    "    plt.plot(df['players_logged_diff'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Outliers\n",
    "\n",
    "We are going to deal with extreme outliers only, and we will continue on with using Logged Data, as this is normalized, on a small scale, and easily transformed back to player count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need to keep a list of dataframes with outliers to use as a testing space.\n",
    "\n",
    "ls_w_outliers = ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls = list(map(lambda df: df.dropna(), ls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outliers for our 'players_logged_differences'\n",
    "\n",
    "plotting_lists = []\n",
    "\n",
    "for df in ls:\n",
    "    outlier_removed = []\n",
    "    Inter_qr = scipy.stats.iqr(df['players_logged_diff'])\n",
    "    mean = df['players_logged_diff'].mean()\n",
    "    Q1 = df['players_logged_diff'].quantile([0.25])[0.25]\n",
    "    Q3 = df['players_logged_diff'].quantile([0.75])[0.75]\n",
    "    upper = Q3 + 3*Inter_qr\n",
    "    lower = Q1 - 3*Inter_qr\n",
    "    print(lower, upper)\n",
    "    for i, val in enumerate(df['players_logged_diff']):\n",
    "        if val > upper:\n",
    "            outlier_removed.append(upper)\n",
    "        elif val < lower:\n",
    "            outlier_removed.append(lower)\n",
    "        else:\n",
    "            outlier_removed.append(val)\n",
    "    plotting_lists.append(outlier_removed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, 4):\n",
    "    ls[i]['outlier_removed_logged_diff'] = plotting_lists[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# let's see how this affected our data\n",
    "\n",
    "for df in ls:\n",
    "    fig = plt.figure(figsize = (15, 8))\n",
    "    plt.plot(df['outlier_removed_logged_diff'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have beautifully stationary data, without outliers, and this is also very easy to return to our original player count!\n",
    "\n",
    "This data is ready to be modelled. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls_prepped = ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Model Construction\n",
    "\n",
    "We will be using a gridsearch to find the optimal model. However, we still have to manually find S, or the seasonal component of the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we define our predictor column and prepare it.\n",
    "\n",
    "mod_ls = []\n",
    "\n",
    "for df in ls:\n",
    "    df_mod = pd.DataFrame()\n",
    "    df_mod['ORLD'] = df['outlier_removed_logged_diff']\n",
    "    mod_ls.append(df_mod)\n",
    "mod_ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we plot both ACF and PACF\n",
    "\n",
    "for df in mod_ls:\n",
    "    acf = plot_acf(df, lags = 25)\n",
    "    pacf = plot_pacf(df, lags = 25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above plots suggest that our seasonal period for all our games is 7 days, or one week. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we establish our parameter variables\n",
    "\n",
    "p = q = d = range(0, 2)\n",
    "pdq = list(itertools.product(p, q, d))\n",
    "pdqs = [(x[0], x[1], x[2], 7) for x in pdq]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ans_ls = []\n",
    "for df in mod_ls:\n",
    "    df = df.dropna()\n",
    "    ans = []\n",
    "    for comb in pdq:\n",
    "        for combs in pdqs:\n",
    "            mod = SARIMAX(df,\n",
    "                                            order=comb,\n",
    "                                            seasonal_order=combs,\n",
    "                                            enforce_stationarity=False,\n",
    "                                            enforce_invertibility=False)\n",
    "\n",
    "            output = mod.fit()\n",
    "            ans.append([comb, combs, output.aic])\n",
    "            print('ARIMA {} x {}12 : AIC Calculated ={}'.format(comb, combs, output.aic))\n",
    "\n",
    "    ans_df = pd.DataFrame(ans, columns=['pdq', 'pdqs', 'aic'])\n",
    "    ans_df = ans_df.loc[ans_df['aic'].idxmin()].dropna()\n",
    "    \n",
    "    ans_ls.append(ans_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, 4):\n",
    "    print(ans_ls[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our optimized models, according to the grid search:\n",
    "\n",
    "CS:GO --- ARIMA(1,0,1)x(1,0,1,7)\n",
    "\n",
    "DOTA 2 --- ARIMA(1,0,1)x(1,1,1,7)\n",
    "\n",
    "ROCKET LEAGUE --- ARIMA(1,0,1)x(1,1,1,7)\n",
    "\n",
    "TEAM FORTRESS 2 --- ARIMA(1,0,1)x(1,0,1,7)\n",
    "\n",
    "### c. Model Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we generate validation statistics for our models\n",
    "\n",
    "csgo_model = SARIMAX(mod_ls[0],\n",
    "                    order = (1,0,1),\n",
    "                    seasonal_order = (1,0,1,7),\n",
    "                    enforce_stationarity = False,\n",
    "                    enforce_invertibility = False)\n",
    "csgo_output = csgo_model.fit()\n",
    "print(csgo_output.summary().tables[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dota_model = SARIMAX(mod_ls[1],\n",
    "                    order = (1,0,1),\n",
    "                    seasonal_order = (1,1,1,7),\n",
    "                    enforce_stationarity = False,\n",
    "                    enforce_invertibility = False)\n",
    "dota_output = dota_model.fit()\n",
    "print(dota_output.summary().tables[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rl_model = SARIMAX(mod_ls[2],\n",
    "                    order = (1,0,1),\n",
    "                    seasonal_order = (1,1,1,7),\n",
    "                    enforce_stationarity = False,\n",
    "                    enforce_invertibility = False)\n",
    "rl_output = rl_model.fit()\n",
    "print(rl_output.summary().tables[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_model = SARIMAX(mod_ls[3],\n",
    "                    order = (1,0,1),\n",
    "                    seasonal_order = (1,0,1,7),\n",
    "                    enforce_stationarity = False,\n",
    "                    enforce_invertibility = False)\n",
    "tf_output = tf_model.fit()\n",
    "print(tf_output.summary().tables[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [csgo_model, dota_model, rl_model, tf_model]\n",
    "outputs = [csgo_output, dota_output, rl_output, tf_output]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, output in enumerate(outputs):\n",
    "    \n",
    "    output.plot_diagnostics(figsize = (10, 10))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Forecasting our Basic Models\n",
    "\n",
    "We will be forecasting from 2020-12-15 to present, and visualizing the full month of December 2020 for comparison to our original values.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we construct predictions\n",
    "\n",
    "predictions = list(map(lambda output: output.get_prediction(start = pd.to_datetime('2020-12-15'), dynamic = False), outputs))\n",
    "pred_conf = list(map(lambda pred: pred.conf_int(), predictions))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = ls_w_outliers[0]['2020-12-01':]['players_logged_diff'].plot(label = \"Truth\")\n",
    "\n",
    "predictions[0].predicted_mean.plot(ax = ax, label = \"Forecasted\", alpha = 0.8)\n",
    "ax.fill_between(pred_conf[0].index,\n",
    "                   pred_conf[0].iloc[:,0],\n",
    "                   pred_conf[0].iloc[:,1], color = 'g', alpha = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = ls_w_outliers[1]['2020-12-01':]['players_logged_diff'].plot(label = \"Truth\")\n",
    "\n",
    "predictions[1].predicted_mean.plot(ax = ax, label = \"Forecasted\", alpha = 0.8)\n",
    "ax.fill_between(pred_conf[1].index,\n",
    "                   pred_conf[1].iloc[:,0],\n",
    "                   pred_conf[1].iloc[:,1], color = 'g', alpha = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = ls_w_outliers[2]['2020-12-01':]['players_logged_diff'].plot(label = \"Truth\")\n",
    "\n",
    "predictions[2].predicted_mean.plot(ax = ax, label = \"Forecasted\", alpha = 0.8)\n",
    "ax.fill_between(pred_conf[2].index,\n",
    "                   pred_conf[2].iloc[:,0],\n",
    "                   pred_conf[2].iloc[:,1], color = 'g', alpha = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = ls_w_outliers[3]['2020-12-01':]['players_logged_diff'].plot(label = \"Truth\")\n",
    "\n",
    "predictions[3].predicted_mean.plot(ax = ax, label = \"Forecasted\", alpha = 0.8)\n",
    "ax.fill_between(pred_conf[3].index,\n",
    "                   pred_conf[3].iloc[:,0],\n",
    "                   pred_conf[3].iloc[:,1], color = 'g', alpha = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, 4):\n",
    "    forecasted = predictions[i].predicted_mean\n",
    "    truth = ls_w_outliers[i][\"2020-01-01\":]['players_logged_diff']\n",
    "    error = forecasted - truth\n",
    "    mse = (error ** 2).mean()\n",
    "    print(mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1 Conclusions:\n",
    "\n",
    "We constructed 4 models from the dataframes we have, and summary statistics show that these can all be said to be viable models in predicting the logged differences in player counts. P-values are lower than 0.05, and mean squared error values are also very low, though this is without any support until we can compare these values to those of other models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Amalgamized Modelling\n",
    "\n",
    "The goal in this section of our project is to construct a model from all of our dataframes available. We will hopefully gain a model that is able to predict  for all of our games to an accuracy comparable to the accuracy values gained for individual models, found in part 1 above. \n",
    "\n",
    "We can construct this model by simply taking the average of all of our dataframe values. In this case we would be forced to scale all values, so we could feasibly use our % change feature for this process.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Taking mean values of dataframes\n",
    "We are going to be taking the mean of '%_chg_players', but we have to prepare this data first. \n",
    "### Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resetting our dataframes to start fresh from when we trimmed the dataframe dates. \n",
    "\n",
    "ls = trimmed_df_list\n",
    "\n",
    "ls = list(map(lambda df: df.dropna(), ls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in order to take the mean, we have to have all our series begin at the same time. We only have to alter team fortress 2\n",
    "\n",
    "ls[3] = ls[3].where(ls[3]['time'] >= \"2016-01-01\").dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualizing this feature\n",
    "for df in ls:\n",
    "    fig = plt.figure(figsize = (15, 8))\n",
    "    plt.plot(df['%chg_players'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we should average our data before removing outliers\n",
    "\n",
    "average_pc = []\n",
    "\n",
    "for i in range(len(ls[0])):\n",
    "    pc = '%chg_players'\n",
    "    average_at_i = np.array([ls[0][pc][i], ls[1][pc][i], ls[2][pc][i], ls[3][pc][i]]).mean() \n",
    "    average_pc.append(average_at_i)\n",
    "df_model = ls[0].copy()\n",
    "df_model[\"mean_%chg_players\"] = average_pc\n",
    "df_model = df_model['mean_%chg_players']\n",
    "\n",
    "df_model_w_outliers = pd.DataFrame(df_model)\n",
    "df_model = pd.DataFrame(df_model)\n",
    "df_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# while our data is already stationary, we have to address outliers, much like we did before with our logged differences. \n",
    "\n",
    "outlier_removed = []\n",
    "\n",
    "IQR = scipy.stats.iqr(df_model)\n",
    "mean = df_model.mean()\n",
    "quantiles = df_model['mean_%chg_players'].quantile([0.25, 0.75])\n",
    "Q1 = quantiles[0.25]\n",
    "Q3 = quantiles[0.75]\n",
    "upper = Q3 + 3*IQR\n",
    "lower = Q1 - 3*IQR\n",
    "for val in df_model['mean_%chg_players']:\n",
    "        if val > upper:\n",
    "            outlier_removed.append(upper)\n",
    "        elif val < lower:\n",
    "            outlier_removed.append(lower)\n",
    "        else:\n",
    "            outlier_removed.append(val)\n",
    "df_model['PCOR'] = outlier_removed\n",
    "\n",
    "df_model = df_model.drop(columns = ['mean_%chg_players'])\n",
    "\n",
    "df_model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspecting how this impacted our visualizations\n",
    "\n",
    "fig = plt.figure(figsize = (15, 8))\n",
    "\n",
    "plt.plot(df_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data is ready to be modelled!\n",
    "\n",
    "As before, we need to first ensure that our seasonality was not affected by our transformations so far. \n",
    "\n",
    "### Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting acf and pacf, showing that our seasonality is retained at s = 7\n",
    "\n",
    "acf = plot_acf(df_model, lags = 25)\n",
    "pacf = plot_pacf(df_model, lags = 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining our parameters for our gridsearch.\n",
    "\n",
    "p = d = q = range(0, 2)\n",
    "pdq = list(itertools.product(p, q, d))\n",
    "pdqs = [(x[0], x[1], x[2], 7) for x in pdq]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "models = []\n",
    "for comb in pdq:\n",
    "    for combs in pdqs:\n",
    "        model = SARIMAX(df_model, \n",
    "                       order = comb,\n",
    "                       seasonal_order = combs,\n",
    "                       enforce_stationarity = False,\n",
    "                       enforce_invertibility = False)\n",
    "        output = model.fit()\n",
    "        models.append([comb, combs, output.aic])\n",
    "        print('ARIMA {} X {} : AIC Calculated = {}'.format(comb, combs, round(output.aic, 2)))\n",
    "models_df = pd.DataFrame(models, columns = ['pdq', 'pdqs', 'aic'])\n",
    "best_model = models_df.loc[models_df['aic'].idxmin()].dropna()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(best_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we generate summary stats for the model\n",
    "\n",
    "mean_model = SARIMAX(df_model_w_outliers,\n",
    "             order = (1,0,1),\n",
    "             seasonal_order = (1,0,1,7),\n",
    "             enforce_stationarity = False,\n",
    "             enforce_invertibility = False)\n",
    "output = mean_model.fit()\n",
    "print(output.summary().tables[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we visualize validation plots\n",
    "\n",
    "output.plot_diagnostics(figsize = (10, 10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Forecasting \n",
    "\n",
    "We will forecast and calculate the mean squared error for each dataframe, using our constructed model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we generate predictions\n",
    "\n",
    "pred = output.get_prediction(start = pd.to_datetime('2020-12-15'))\n",
    "pred_conf = pred.conf_int()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and visualize the forecasts for each of our dataframes\n",
    "\n",
    "ax = ls_w_outliers[0]['2020-12-01':]['%chg_players'].plot(label = \"Truth\")\n",
    "\n",
    "pred.predicted_mean.plot(ax = ax, label = \"Forecasted\", alpha = 0.8)\n",
    "ax.fill_between(pred_conf.index,\n",
    "                   pred_conf.iloc[:,0],\n",
    "                   pred_conf.iloc[:,1], color = 'g', alpha = 0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = ls_w_outliers[1]['2020-12-01':]['%chg_players'].plot(label = \"Truth\")\n",
    "\n",
    "pred.predicted_mean.plot(ax = ax, label = \"Forecasted\", alpha = 0.8)\n",
    "ax.fill_between(pred_conf.index,\n",
    "                   pred_conf.iloc[:,0],\n",
    "                   pred_conf.iloc[:,1], color = 'g', alpha = 0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = ls_w_outliers[2]['2020-12-01':]['%chg_players'].plot(label = \"Truth\")\n",
    "\n",
    "pred.predicted_mean.plot(ax = ax, label = \"Forecasted\", alpha = 0.8)\n",
    "ax.fill_between(pred_conf.index,\n",
    "                   pred_conf.iloc[:,0],\n",
    "                   pred_conf.iloc[:,1], color = 'g', alpha = 0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = ls_w_outliers[3]['2020-12-01':]['%chg_players'].plot(label = \"Truth\")\n",
    "\n",
    "pred.predicted_mean.plot(ax = ax, label = \"Forecasted\", alpha = 0.8)\n",
    "ax.fill_between(pred_conf.index,\n",
    "                   pred_conf.iloc[:,0],\n",
    "                   pred_conf.iloc[:,1], color = 'g', alpha = 0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finally, we calculate the MSE for each of these forecasts. \n",
    "\n",
    "for df in ls_w_outliers:\n",
    "    forecasted = pred.predicted_mean\n",
    "    truth = df[\"2020-12-15\":]['%chg_players']\n",
    "    mse = ((forecasted - truth) ** 2).mean()\n",
    "    print(mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusions on Amalgamized Modelling\n",
    "\n",
    "We constructed one model from the mean change in players for all of our games. This model's summary statistics show that it is more viable than our basic models in part 1, due to the fact that our validation visualizations, namely the QQ plot suggesting that residuals in this model are more normally distributed than for all other models constructed thus far.\n",
    "\n",
    "This model also has the benefit of being able to be applied to any game. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Model Construction from Control Dataframe, Team Fortress 2\n",
    "\n",
    "In this section, we will be constructing a model solely from Team Fortress 2, our control dataframe. We called this game our 'control' becuase there is a large amount of data available (13 years of daily player counts), as well as the fact that there are limited events for Team Fortress 2.\n",
    "\n",
    "We would like to see if trends from one game are able to be used to predict trends on another, unrelated game. If this model performs well, we can say that trends from one game predict those of others. This seems to be the case from Part 2, since our amalgamized model performed well. \n",
    "\n",
    "Here, we are only using one game, which cuts down on the required data for our model. This is also more realistic in a business sense, as a company would want to be able to predict competitor trends based on their own trends. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will be predicting logged differences for ease of comparison across models. \n",
    "\n",
    "# using our benchmarked lists, which contain all constructed columns\n",
    "\n",
    "tf = ls_prepped[3]\n",
    "\n",
    "tf_model_df = pd.DataFrame()\n",
    "\n",
    "tf_model_df['ORLD'] = tf['outlier_removed_logged_diff']\n",
    "\n",
    "tf_model_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can jump straight to model construction in this part. \n",
    "\n",
    "p = q = d = range(0,2)\n",
    "pdq = list(itertools.product(p,d,q))\n",
    "pdqs = [(x[0], x[1], x[2], 7) for x in pdq]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "mods = []\n",
    "for comb in pdq:\n",
    "    for combs in pdqs:\n",
    "        model = SARIMAX(tf_model_df, \n",
    "                       order = comb,\n",
    "                       seasonal_order = combs,\n",
    "                       enforce_stationarity = False,\n",
    "                       enforce_invertibility = False)\n",
    "        output = model.fit()\n",
    "        mods.append([comb, combs, output.aic])\n",
    "        print('ARIMA {} X {} : AIC Calculated = {}'.format(comb, combs, round(output.aic, 2)))\n",
    "models_df = pd.DataFrame(mods, columns = ['pdq', 'pdqs', 'aic'])\n",
    "best_model = models_df.loc[models_df['aic'].idxmin()].dropna()\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(best_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "control_model = SARIMAX(tf_model_df,\n",
    "                       order = (1,0,1),\n",
    "                       seasonal_order = (1,0,1,7),\n",
    "                       enforce_stationarity = False,\n",
    "                       enforce_invertibility = False)\n",
    "control_output = control_model.fit()\n",
    "print(control_output.summary().tables[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = control_output.get_prediction(start = pd.to_datetime('2020-12-15'))\n",
    "pred_conf = pred.conf_int()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax =ls_w_outliers[0][\"2020-12-01\":]['players_logged_diff'].plot(label = \"Truth\")\n",
    "\n",
    "pred.predicted_mean.plot(ax = ax, label = \"Forecasted\", alpha = 0.8)\n",
    "ax.fill_between(pred_conf.index,\n",
    "                   pred_conf.iloc[:,0],\n",
    "                   pred_conf.iloc[:,1], color = 'g', alpha = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax =ls_w_outliers[1][\"2020-12-01\":]['players_logged_diff'].plot(label = \"Truth\")\n",
    "\n",
    "pred.predicted_mean.plot(ax = ax, label = \"Forecasted\", alpha = 0.8)\n",
    "ax.fill_between(pred_conf.index,\n",
    "                   pred_conf.iloc[:,0],\n",
    "                   pred_conf.iloc[:,1], color = 'g', alpha = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax =ls_w_outliers[2][\"2020-12-01\":]['players_logged_diff'].plot(label = \"Truth\")\n",
    "\n",
    "pred.predicted_mean.plot(ax = ax, label = \"Forecasted\", alpha = 0.8)\n",
    "ax.fill_between(pred_conf.index,\n",
    "                   pred_conf.iloc[:,0],\n",
    "                   pred_conf.iloc[:,1], color = 'g', alpha = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax =ls_w_outliers[3][\"2020-12-01\":]['players_logged_diff'].plot(label = \"Truth\")\n",
    "\n",
    "pred.predicted_mean.plot(ax = ax, label = \"Forecasted\", alpha = 0.8)\n",
    "ax.fill_between(pred_conf.index,\n",
    "                   pred_conf.iloc[:,0],\n",
    "                   pred_conf.iloc[:,1], color = 'g', alpha = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and we generate mse for each df. the last is best because this is tf model testing on tf data.\n",
    "\n",
    "for df in ls_w_outliers:\n",
    "    forecasted = pred.predicted_mean\n",
    "    truth = df[\"2020-12-15\":]['%chg_players']\n",
    "    mse = ((forecasted - truth) ** 2).mean()\n",
    "    print(mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusions on our Control Model\n",
    "\n",
    "While this model constructed solely from Team Fortress 2 data can be said to be viable from our summary stats, our mean squared error is much higher than our amalgamized model. This suggests that Team Fortress 2 is not as effective at predicting trends than an amalgamization of all of our data being used to predict trends. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we save all of our constructed models for use in the next notebook\n",
    "\n",
    "import pickle\n",
    "\n",
    "models = [csgo_model,\n",
    "dota_model,\n",
    "rl_model,\n",
    "tf_model,\n",
    "mean_model]\n",
    "\n",
    "names = [\"models/csgo_model.pkl\",\n",
    "\"models/dota_model.pkl\",\n",
    "\"models/rl_model.pkl\",\n",
    "\"models/tf_model.pkl\",\n",
    "\"models/mean_model.pkl\"]\n",
    "\n",
    "for i in range(len(models)):\n",
    "    with open(names[i], 'wb') as file:\n",
    "        pickle.dump(models[i], file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we also save our testing dataframes, those that retain outliers. \n",
    "\n",
    "names = ['csgo_test.csv',\n",
    "        'dota_test.csv',\n",
    "        'rl_test.csv',\n",
    "        'tf_test.csv']\n",
    "\n",
    "for i in range(len(ls_w_outliers)):\n",
    "    ls_w_outliers[i].to_csv('data/Test/'+names[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next:\n",
    "\n",
    "In the next notebook, we will compare our models and describe the pros and cons of each, while also using models to find interesting points in the future. We will also provide our recommendations to those looking to utilize our findings, and conclude with thoughts on how our investigation could have yeilded more insight into these data. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learn-env",
   "language": "python",
   "name": "learn-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
